# EE 360C Lecture -- 4.4.17

## Today: Dynamic Programming

### Review of Algorithmic Techniques

- *Greedy*: Build up a solution incrementally, myopically optimizing some local
  criterion

- *Divide and Conquer*: Break up a problem into two (or more) subproblems, solve
  each subproblem independently, and combine the solutions to the subproblems
  to form a solution to the original problem.

- *Dynamic Programming*: Break up a problem into a series of overlapping
  subproblems and build up solutoins to larger and larger subproblems

### What Is Dynamic Programming?

- An algorithm design technique that relies on soultions to subproblems to
  solve a problem.

    - In DP, subproblems aren't independent

    - DP algorithm solves each of these subproblems just once, saving time in
      comparison to a traditional divide and conquer approach

- Commonly used for optimization problems in which many possible solutions
  exist; the algo helps us find one of the possibilities.

    - Every solution has a value

    - Algorithm helps us find a solution with the optimial value

Usually follows these steps:

1. Characterize the structure of an optimal solution

2. Recursively define the value of an optimal solution

3. Compute the value of an optimal solution in a bottom-up fashion

4. Construct an optimal solution from the computed information

#### Example: Weighted Interval Scheduling

Interval scheduling problem where each job has a different value. The new goal:
find the maximum weight subset of mutually compatible jobs.

```
Example Weighted Schedule
=========================

    v=2
1 |-------|                     p(1) = 0
          v=4
2      |---------|              p(2) = 0
              v=4
3          |--------|           p(3) = 1
            v = 7
4     |------------------|      p(4) = 0
                       v=2
5                    |------|   p(5) = 3
                        v=1
6                      |------| p(6) = 3
```

- Label and sort jobs by finishing time. Define `p(j)` to be the largest index
  `i < j` such that job `i` is compatible with job `j`.

- `OPT(j)` is the value of the optimal solution to the problem consisting of
  job request `1,2,...,j`.

    - Case 1: `OPT` selects job `j`.

        - then `OPT` can't include any of the jobs in the range `p(j) + 1, p(j)
          + 2, ...,j-1`.

    - Case 2: `OPT` doesn't include `j`

        - `OPT` must include the optimal solution to the proble consisting o
          remaining compatible jobs `1,2,...,j-1`.

- `OPT(j) = 0 if j=0 or max(v_i+OPT(p(j)), OPT(j-1))`

- The brute force recursive solution to this problem duplicates a bunch of
  subproblems.

- Memoization: Store the results of each sub-problem in a cache; look them up
  as needed.

    - The effective algorithm is the same as the recursive brute force algo,
      but each solution is stored and keyed by the index `j`. If the solution
      already exists, just used the stored solution. In the initialization,
      zero is stored at index 0 to represent the bottom-out portion of the
      recursive call.

- Runtime of memoized compute-opt is O(n)

- To get the solution from this optimal value array `M`,

    ```
        Find-Solution(j) {
            if (j==0)
                output nothing
            else if (v_j + M[p(j)] > M[j-1])
                print j
                Find-Solution(p(j))
            else
                Find-Solution(j-1)
        }
    ```

- **Plot twist, this wasn't dynamic programming**

- The dynamic programming solution is simply unwinding the recursion into a
  simple loop

#### When does dynamic programming apply?

Problems should exhibit:

- Optimal substructure

    - Optimal substructure may also be a good candidate for greedy solution
      instead of dynamic programming if the subproblems are independent of each
      other

- Overlapping problems

    - If recursive solution is going to solve the same subproblem multiple
      times, then we're wasting computation

        - One algernative for overlapping subproblems is *memoization*

        - It's like dynamic programming but fills in the table using a top-down
          approach and thus is more like traditional recursion

        - Results can then just be computed once and looked up thereafter

    - Dynamic programming is "bottom-up" instead of "top-down" like recursion

#### Segmented least squares example

Given `n` points in the plane, find a line `y = ax + b` that minimizes the sum
of the squared error.

$$ SSE = \sum_{n=1}^{n}(y_i - ax_i - b)^2$$

What is a reasonable choice for `f(x)` to balance accuracy (goodness of fit)
and parsimony (number of lines)?

- Tradeoff function E + cL for some constant c (E = error, L = lines)
