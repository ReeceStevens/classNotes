# EE 360C Lecture -- 3.21.17

## Greedy Algorithms (cont'd)

From last time: Minimum-spanning trees (MST)

### In class exercise

Let G be a weighted, undirected graph where the edge weights are distinct. An
edge is *dangerous* if it is the longest edge in some cycle, and an edge is
*useful* if it does not belong to any cycle in G.

- Prove that any MST of G contains every useful edge

    - If there is no cycle between nodes `u` and `v`, then the edges between
      `u` and `v` are the only possible path to get from `u` to `v`. Therefore,
      they must exist in the MST to satisfy the spanning property.

- Prove that any MST of G contains no dangerous edges

    - If a cycle exists between nodes `u` and `v`, then there are at least two
      sets of edges that connect the two nodes. Therefore, for a minimum
      spanning tree, we would never select a dangerous edge, since by selecting
      any other shorter edge in a cycle we would produce a shorter spanning
      tree.

- Both of these proofs are formally proven by contradiction

### Greedy Strategy in General

We are direct in applying the greedy method:

- Cast the optimization problem into one in which we make a choice and are left
  with only a single subproblem to solve

    - Prove that there is always an optimal solution to the orignial problem that
    makes the greedy choice

    - Demonstrate that combining a solution to the remaining subproblem with
    the greedy choice yields an optimal solution to the original problem


### The Knapsack problem

Two different versions:

- 0-1 Knapsack

    - Binary decision-- you can only choose to take a whole item or none of it

    - Optimal substructure property? Yes

    - *Doesn't* stisfy the greedy-choice property-- it's possible that the
      greedy choice for per-pound value is not the most optimal choice

- Fractional Knapsack

    - You can take a fraction of an item

    - Optimal substructure property? Yes

### Huffman Codes

Huffman coding is a variable length data compression scheme that takes
advantage of the fact that some characters are more common than others

- More common characters get shorter codes

- Avaoid ambiguity by requiring that no codeword is a prefix of another

To implement this, we need a fairly efficient representation of the code book
that we can traverse quickly.

Use a full binary tree whose leaves are characters represented by the code.

- the "optimal" encoding is a full binary tree; if not, we could "splice" out a
  node that only had one child and get shorter paths from the root to the
  leaves in that subtree.

- Given a codetree `T`, the number of bits needed to encode the file is:

$$ B(T) = \sum_{c\in C} f(c) d_T(c)$$

  where $d_T(C)$ is the depth of `c`'s leaf in `T`

What if we knew the tree structure of the optimal code?

- Frequency of a lower character must be less than or equal to the frequency
  of a higher character

If we don't know:

- Start with C leaves, perform C-1 merging operations to create the tree

    - use a min heap keyed on F to merge the two least frequent objects

    - The result is a new object with frequency that's the sum of the
      frequencies of the merged objects

**Go back over the Huffman coding slides, this seems a little subtle**

Correctness proof:

- T is a full binary tree, so it must have at least two deepest leaves tat are
  siblings. Since x and y have the lowerst frequencies, we can swap them into
  these two spots without increasing the total cost of encoding

- By choosing x and y with the lowest frequencies, Huffman picks the least cost
  merge at each step.
