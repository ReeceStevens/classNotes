# EE 360C Lecture -- 3.28.17

## Announcements

- Exam is in a different room than before (WCH, but check to be sure)

- One page, front and back, of cheat-sheet notes

## Master Method (cont'd)

Purpose of the master method: finding the running time of recursive algorithms.

$$ T(n) = aT(n/b) + \Theta(n^l (lg n)^k) $$
$$ T(c) = \Theta(1), c = const. $$

## Multiplication Case Study

Consider the traditional iterative "ripple carry" algorithm for addition. What
is its running time for adding two n-digit numbers? `O(n)`

Common algorithm for multiplying two n-digit numbers? `O(n^2)`

### Recursive Multiplication 1

Group digits in different powers of 10, factoring it out.

Running time:

$$ T(n) = 4T(n/2) + O(n) $$

Implies master method, case I, evaluating to `O(n^2)`. No good, just as bad as
the first. `:(`

However, you can drop one of the four multiplications by doing some algebra and
combining the polynomial `(a+b)(c+d)`.

### Matrix multiplication (skipped, go back later)

### Recursion Trees

In a *recursion tree*, each node representst the cost of a single subproblem
somewhere in the set of recursive function invocations.

- Sum the nodes in each level to get a per-level cost

- Get the total running time by summing all of the levels

#### Merge Sort Recurrence Exercise

Formulate the merge sort recurrence: T(n) = 2T(floor(n/2)) + Theta(n)

a = number of subproblems, b = fraction of whole used for each subproblem

a = 2, b = 2, l = 1, k = 0 => $T(n) = [sum_0^{log_2n-1} 2^i \Theta(n/2^i) ]+ cn$

=> `O(nlog(n))`

Another example:

T(n) = 3T(floor(n/4)) + Theta(n^2)

a = 3, b = 4, l = 2, k = 0 =>

$$ T(n) = [\sum_0^{log_2n-1} 3^i * f(n/4^i) ] + \Theta(n^(log_4(3))) $$

This doesn't reduce nicely. Lots of algebra. At the end of the day, we get
`O(n^2)` by doing some geometric series expansion trickery.

Verified by a proof by induction.

Solve merge sort recurrence by induction:

- Base case: n = 1, trivially true

- Inductive step: T(n) < c(nlogn) for some constant c.

Assume true for n/2; T(n/2) < c(n/2 * log(n/2))

    T(n) < 2*c(n/2)*log(n/2) + Theta(n)

    T(n) < c*n*log(n/2) + Theta(n)

    T(n) < n*log(n)
