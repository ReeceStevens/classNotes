# EE 360C Lecture - 3.23.17

## Divide and Conquor Algorithms

### Algorithm Design

- *Incremental design*: given a sorted subarray, insertion sort inserts a
  single element into its proper place, generating a longer sorted subarray

- *Greedy choice*: frame a problem as a choice, make the greedy choice

- *Divide and conquer*: break the problem into several subproblems that are
  similar to but smaller than the original, then solve them recursively. Then,
  combine the small solutions together to get the original problem's solution.

    - Divide

    - Conquer

    - Combine

### Example: Exponentiation

Given `a` and positive integer `n`, consider the problem of calculating the
expression `a**n`.

```
// Slow Power, O(n)
x <- a
for i <- 2 to n
    do x <- x * a
return x
```

```
// FastPower(a,n), O(logn)
if n = 1
    return a
else
    x <- FastPower(a, floor(n/2))
    if n is even
        then return x*x
    else
        return x*x*a
```

### Example: Merge Sort

Sort an n-element sequence

- Divide: divide n element sequence to be sorted into the two subsequences of
  n/2 elements each

- Conquer: sort the two subsequences recursively using merge-sort

- Combine: Merge the two sorted subsequences to produce the sorted answer.

Base case: if the sequence is one element long, it's already sorted

Running Time:

- The actual recursive merge step is Theta(n)

- When applied recursively on an unsorted array, O(nlog(n))

    - Calculated by a recurrence relation

        T(n) = aT(n/b) + D(n) + C(n) if n > c
             = Theta(1) if n <= c

        Divide: find midpoint of array, Theta(1)

        Conquer: Define two suproblems of size n/2, so 2T(n/2)

        Combine: Merge takes Theta(n)

        Therefore: T(n) = 2T(n/2) + Theta(n) if n > 1

        => Theta(nlogn)

### The Master Method

Consider T(n) = aT(n/b) + f(n), where a >= 1 and b > 1 and f(n) is an
asymptotically positive function.

If f(n) is polynomially different from n^(log_b(a)), runtime can be determined

Example:

- T(n) = 2T(n/2) + Theta(n)

    - a = 2, b = 2, f(n) = O(n). Which case is it?

    - n^(log_b(a)) = n^(log_2(2)) = n. Case 2, Tightly bound to each other

    - T(n) = Theta(n^(log_b(a)logn)) = Theta(nlogn) <-- this is how we find the
      time complexity of merge sort

- T(n) = 9T(n/3) + n

    - a = 9, b = 3, f(n) = n. Which case is it?

    - n^(log_b(a)) = n^2, then f(n) = O(n^(log_3(9) - epsilon) for epsilon = 1.
      Case 1.

    - Using master method, running time is Theta(n^(log_b(a))) = Theta(n^2)
